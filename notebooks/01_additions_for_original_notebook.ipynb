{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd44611",
   "metadata": {},
   "source": [
    "# Addendum Cells — Mentor Feedback (Append-Only)\n",
    "\n",
    "> Paste these cells **after the last cell** of `01_train_decision_tree.ipynb`.\n",
    "> They will: (1) reuse existing variables/paths if present; (2) otherwise fall back\n",
    "> to sensible defaults. Nothing in prior cells is modified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A0. Safe Imports and Paths (non-destructive)\n",
    "import os, json, math, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optional\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except Exception:\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "# Reuse existing paths if they exist in globals; otherwise set defaults\n",
    "try:\n",
    "    ARTIFACTS\n",
    "except NameError:\n",
    "    ARTIFACTS = Path(\"artifacts\")\n",
    "\n",
    "try:\n",
    "    FIGS\n",
    "except NameError:\n",
    "    FIGS = Path(\"reports\") / \"figures\"\n",
    "\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    RANDOM_STATE\n",
    "except NameError:\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "print(\"Using ARTIFACTS ->\", ARTIFACTS)\n",
    "print(\"Using FIGS      ->\", FIGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0141c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A1. Bootstrap Data/Columns from prior cells if missing\n",
    "# Try to reuse df, target, date_col, preprocessor if defined; else infer.\n",
    "\n",
    "if 'df' not in globals():\n",
    "    # Fallback: try common data paths from the repo\n",
    "    CANDS = [\n",
    "        Path(\"data\") / \"mess_waste_GIM_500.csv\",\n",
    "        Path(\"mess_waste_GIM_500.csv\"),\n",
    "        Path(\"data\") / \"mess_waste_GIM_daily_exams.csv\",\n",
    "    ]\n",
    "    src = next((p for p in CANDS if p.exists()), None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"Could not find data. Please set df manually or place CSV under data/.\")\n",
    "    df = pd.read_csv(src)\n",
    "\n",
    "# Detect/confirm target and date/event columns\n",
    "POSSIBLE_TARGETS = [\"food_waste_kg\", \"waste_kg\", \"leftover_kg\", \"wastage\"]\n",
    "DATE_CANDIDATES = [\"date\", \"day\", \"timestamp\", \"dt\"]\n",
    "EVENT_CANDIDATES = [\"event\", \"event_type\", \"special_event\"]\n",
    "\n",
    "if 'target' not in globals() or target not in df.columns:\n",
    "    target = next((c for c in POSSIBLE_TARGETS if c in df.columns), None)\n",
    "    if target is None:\n",
    "        raise KeyError(f\"Set target variable. Expected one of {POSSIBLE_TARGETS} in df.\")\n",
    "\n",
    "if 'date_col' not in globals() or date_col not in df.columns:\n",
    "    date_col = next((c for c in DATE_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "if date_col is not None:\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "event_col = next((c for c in EVENT_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "# Build/Reuse a robust preprocessor if none defined\n",
    "if 'preprocessor' not in globals():\n",
    "    X_tmp = df.drop(columns=[target])\n",
    "    num_cols = X_tmp.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    if date_col in num_cols: num_cols.remove(date_col)\n",
    "    if date_col in cat_cols: cat_cols.remove(date_col)\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ])\n",
    "\n",
    "print(\"Columns detected. target:\", target, \"| date_col:\", date_col, \"| event_col:\", event_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A2. Metrics & Utilities\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    out = np.zeros_like(denom)\n",
    "    mask = denom != 0\n",
    "    out[mask] = diff[mask] / denom[mask]\n",
    "    return 100.0 * np.mean(out)\n",
    "\n",
    "def seasonal_naive(y_series, period=7):\n",
    "    y = np.asarray(y_series)\n",
    "    y_hat = np.full_like(y, np.nan, dtype=float)\n",
    "    for t in range(period, len(y)):\n",
    "        y_hat[t] = y[t - period]\n",
    "    return y_hat\n",
    "\n",
    "def interval_coverage(y_true, lo, hi):\n",
    "    y = np.asarray(y_true)\n",
    "    return float(np.mean((y >= lo) & (y <= hi)))\n",
    "\n",
    "def waste_shortage(prepared, actual):\n",
    "    prepared = np.asarray(prepared)\n",
    "    actual = np.asarray(actual)\n",
    "    waste = np.maximum(0.0, prepared - actual)\n",
    "    shortage = np.maximum(0.0, actual - prepared)\n",
    "    return waste, shortage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A3. Rolling-Origin Backtest (RF) vs Seasonal-Naive\n",
    "# Expanding window, H=1 day. Saves overall and monthly MAE/SMAPE.\n",
    "\n",
    "# Feature engineering (light) — non-destructive to df\n",
    "df_fe = df.copy()\n",
    "if date_col is not None:\n",
    "    d = df_fe[date_col].dt\n",
    "    df_fe[\"dow\"] = d.dayofweek\n",
    "    df_fe[\"month\"] = d.month\n",
    "    df_fe[\"is_weekend\"] = (d.dayofweek >= 5).astype(int)\n",
    "    for win in (3, 7):\n",
    "        df_fe[f\"roll_mean_{win}\"] = df_fe[target].rolling(win, min_periods=1).mean().shift(1)\n",
    "        df_fe[f\"roll_std_{win}\"]  = df_fe[target].rolling(win, min_periods=1).std().shift(1)\n",
    "\n",
    "y = df_fe[target].astype(float).values\n",
    "X = df_fe.drop(columns=[target])\n",
    "\n",
    "rf_bt = RandomForestRegressor(n_estimators=400, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_pipe = Pipeline([(\"prep\", preprocessor), (\"rf\", rf_bt)])\n",
    "\n",
    "y_naive = seasonal_naive(y, period=7)\n",
    "preds_rf = np.full_like(y, np.nan, dtype=float)\n",
    "\n",
    "start = 14  # two-week warmup\n",
    "for t in range(start, len(df_fe)-1):\n",
    "    train_idx = slice(0, t+1)\n",
    "    test_idx = t+1\n",
    "    rf_pipe.fit(X.iloc[train_idx], y[train_idx])\n",
    "    preds_rf[test_idx] = rf_pipe.predict(X.iloc[[test_idx]])[0]\n",
    "\n",
    "mask = ~np.isnan(preds_rf) & ~np.isnan(y_naive)\n",
    "overall = {\n",
    "    \"RF_MAE\": float(mean_absolute_error(y[mask], preds_rf[mask])),\n",
    "    \"RF_SMAPE\": float(smape(y[mask], preds_rf[mask])),\n",
    "    \"Naive_MAE\": float(mean_absolute_error(y[mask], y_naive[mask])),\n",
    "    \"Naive_SMAPE\": float(smape(y[mask], y_naive[mask])),\n",
    "}\n",
    "overall[\"Improvement_MAE_%\"] = 100.0 * (1 - overall[\"RF_MAE\"] / overall[\"Naive_MAE\"])\n",
    "overall[\"Improvement_SMAPE_%\"] = 100.0 * (1 - overall[\"RF_SMAPE\"] / overall[\"Naive_SMAPE\"])\n",
    "\n",
    "months = (df_fe[date_col].dt.to_period(\"M\").astype(str).values\n",
    "          if date_col is not None else np.array([\"ALL\"]*len(df_fe)))\n",
    "\n",
    "bt_df = pd.DataFrame({\"month\": months, \"y\": y, \"rf\": preds_rf, \"naive\": y_naive}).dropna()\n",
    "\n",
    "monthly_rows = []\n",
    "for m, g in bt_df.groupby(\"month\"):\n",
    "    monthly_rows.append({\n",
    "        \"month\": m,\n",
    "        \"RF_MAE\": float(mean_absolute_error(g[\"y\"], g[\"rf\"])),\n",
    "        \"RF_SMAPE\": float(smape(g[\"y\"], g[\"rf\"])),\n",
    "        \"Naive_MAE\": float(mean_absolute_error(g[\"y\"], g[\"naive\"])),\n",
    "        \"Naive_SMAPE\": float(smape(g[\"y\"], g[\"naive\"])),\n",
    "    })\n",
    "monthly_df = pd.DataFrame(monthly_rows).sort_values(\"month\")\n",
    "\n",
    "pd.DataFrame([overall]).to_csv(ARTIFACTS / \"backtest_overall.csv\", index=False)\n",
    "monthly_df.to_csv(ARTIFACTS / \"backtest_monthly.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"backtest_overall.csv\")\n",
    "print(\"Saved:\", ARTIFACTS / \"backtest_monthly.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A4. Quantile Regression Decision Layer (0.1/0.5/0.9) + Coverage + Policy\n",
    "q_models = {}\n",
    "for q in [0.1, 0.5, 0.9]:\n",
    "    q_models[q] = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"gbr\", GradientBoostingRegressor(loss=\"quantile\", alpha=q, random_state=RANDOM_STATE, n_estimators=400, max_depth=3))\n",
    "    ])\n",
    "\n",
    "q_lo = np.full_like(y, np.nan, dtype=float)\n",
    "q_md = np.full_like(y, np.nan, dtype=float)\n",
    "q_hi = np.full_like(y, np.nan, dtype=float)\n",
    "\n",
    "for t in range(start, len(df_fe)-1):\n",
    "    train_idx = slice(0, t+1)\n",
    "    test_idx = t+1\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        q_models[q].fit(X.iloc[train_idx], y[train_idx])\n",
    "    q_lo[test_idx] = q_models[0.1].predict(X.iloc[[test_idx]])[0]\n",
    "    q_md[test_idx] = q_models[0.5].predict(X.iloc[[test_idx]])[0]\n",
    "    q_hi[test_idx] = q_models[0.9].predict(X.iloc[[test_idx]])[0]\n",
    "\n",
    "mask_q = ~np.isnan(q_lo) & ~np.isnan(q_md) & ~np.isnan(q_hi)\n",
    "coverage90 = float(np.mean((y[mask_q] >= q_lo[mask_q]) & (y[mask_q] <= q_hi[mask_q])))\n",
    "with open(ARTIFACTS / \"interval_coverage.json\", \"w\") as f:\n",
    "    json.dump({\"coverage90\": coverage90}, f, indent=2)\n",
    "print(\"Saved:\", ARTIFACTS / \"interval_coverage.json\")\n",
    "\n",
    "# Policy simulation across lambda in [0,1]\n",
    "lams = np.linspace(0, 1, 6)\n",
    "policy = []\n",
    "for lam in lams:\n",
    "    prepared = q_md.copy()\n",
    "    prepared[mask_q] = q_md[mask_q] + lam * (q_hi[mask_q] - q_md[mask_q])\n",
    "    waste = np.maximum(0, prepared[mask_q] - y[mask_q])\n",
    "    shortage = np.maximum(0, y[mask_q] - prepared[mask_q])\n",
    "    policy.append({\n",
    "        \"lambda\": float(lam),\n",
    "        \"avg_waste\": float(np.mean(waste)),\n",
    "        \"avg_shortage\": float(np.mean(shortage)),\n",
    "        \"efficiency_%\": float(100.0 * (1 - (np.mean(waste) / (np.mean(y[mask_q]) + 1e-8))))\n",
    "    })\n",
    "pd.DataFrame(policy).to_csv(ARTIFACTS / \"policy_simulation.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"policy_simulation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A5. Event Effects + Hierarchical Pooling Fallback\n",
    "evt = event_col if event_col in df_fe.columns else None\n",
    "if evt is None:\n",
    "    df_fe[\"event_synth\"] = \"Normal\"\n",
    "    evt = \"event_synth\"\n",
    "\n",
    "bt_ev = pd.DataFrame({\n",
    "    \"date\": df_fe[date_col] if date_col is not None else pd.RangeIndex(len(df_fe)),\n",
    "    \"y\": y,\n",
    "    \"rf\": pd.Series(np.where(np.isnan(q_md), np.nan, q_md)).fillna(method=\"ffill\"),  # use median as a stable predictor if RF preds not aligned\n",
    "    \"event\": df_fe[evt]\n",
    "}).dropna()\n",
    "\n",
    "bt_ev[\"resid\"] = bt_ev[\"y\"] - bt_ev[\"rf\"]\n",
    "evt_stats = bt_ev.groupby(\"event\")[\"resid\"].agg([\"mean\",\"std\",\"count\"]).reset_index().rename(columns={\"mean\":\"resid_mean\",\"std\":\"resid_std\",\"count\":\"n\"})\n",
    "evt_stats[\"global_mean\"] = bt_ev[\"resid\"].mean()\n",
    "\n",
    "k = max(1.0, evt_stats[\"n\"].median())\n",
    "evt_stats[\"shrink_mean\"] = (evt_stats[\"n\"]/(evt_stats[\"n\"]+k))*evt_stats[\"resid_mean\"] + (k/(evt_stats[\"n\"]+k))*evt_stats[\"global_mean\"]\n",
    "evt_stats.to_csv(ARTIFACTS / \"event_residuals.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"event_residuals.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27adc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A6. Explainability (SHAP if available, else Permutation) + Top Errors\n",
    "rf_full = Pipeline([(\"prep\", preprocessor), (\"rf\", RandomForestRegressor(n_estimators=600, random_state=RANDOM_STATE, n_jobs=-1))])\n",
    "rf_full.fit(X, y)\n",
    "\n",
    "# Feature names\n",
    "feat_names = []\n",
    "try:\n",
    "    feat_names += list(preprocessor.transformers_[0][2])  # numeric\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "    base = preprocessor.transformers_[1][2]\n",
    "    feat_names += ohe.get_feature_names_out(base).tolist()\n",
    "except Exception:\n",
    "    feat_names = [f\"f{i}\" for i in range(rf_full.named_steps['rf'].n_features_in_)]\n",
    "\n",
    "importance_df = None\n",
    "if SHAP_AVAILABLE:\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(rf_full.named_steps[\"rf\"])\n",
    "        X_tr = preprocessor.fit_transform(X)\n",
    "        idx = np.linspace(0, X_tr.shape[0]-1, min(500, X_tr.shape[0])).astype(int)\n",
    "        sv = explainer.shap_values(X_tr[idx])\n",
    "        shap_abs = np.mean(np.abs(sv), axis=0)\n",
    "        importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": shap_abs}).sort_values(\"importance\", ascending=False)\n",
    "    except Exception as e:\n",
    "        print(\"SHAP error; falling back to permutation:\", e)\n",
    "\n",
    "if importance_df is None:\n",
    "    perm = permutation_importance(rf_full, X, y, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": perm.importances_mean}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "importance_df.to_csv(ARTIFACTS / \"feature_importance.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"feature_importance.csv\")\n",
    "\n",
    "# Top mispredictions using available predictions (prefer rolling-origin rf if present, else median quantiles)\n",
    "pred_series = pd.Series(np.where(np.isnan(q_md), np.nan, q_md), name=\"pred\").fillna(method=\"ffill\")\n",
    "err_df = pd.DataFrame({\n",
    "    \"date\": df_fe[date_col] if date_col is not None else pd.RangeIndex(len(df_fe)),\n",
    "    \"y\": y,\n",
    "    \"pred\": pred_series.values\n",
    "}).dropna()\n",
    "err_df[\"abs_err\"] = np.abs(err_df[\"y\"] - err_df[\"pred\"])\n",
    "TOPK = 15\n",
    "context_cols = [c for c in df_fe.columns if c != target]\n",
    "top_err = err_df.sort_values(\"abs_err\", ascending=False).head(TOPK).merge(df_fe[context_cols], left_index=True, right_index=True, how=\"left\")\n",
    "top_err.to_csv(ARTIFACTS / \"top_mispredictions.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"top_mispredictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A7. Plots for Overleaf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Monthly MAE/SMAPE\n",
    "try:\n",
    "    monthly_df = pd.read_csv(ARTIFACTS / \"backtest_monthly.csv\")\n",
    "    if not monthly_df.empty:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(monthly_df[\"month\"], monthly_df[\"RF_MAE\"], marker='o', label=\"RF MAE\")\n",
    "        plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_MAE\"], marker='o', label=\"Naive MAE\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(\"Monthly MAE — Rolling-Origin Backtest\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        p1 = FIGS / \"monthly_mae.png\"\n",
    "        plt.savefig(p1, dpi=150)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", p1)\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(monthly_df[\"month\"], monthly_df[\"RF_SMAPE\"], marker='o', label=\"RF SMAPE\")\n",
    "        plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_SMAPE\"], marker='o', label=\"Naive SMAPE\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(\"Monthly SMAPE — Rolling-Origin Backtest\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        p2 = FIGS / \"monthly_smape.png\"\n",
    "        plt.savefig(p2, dpi=150)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", p2)\n",
    "except Exception as e:\n",
    "    print(\"Monthly plots skipped:\", e)\n",
    "\n",
    "# Quantile interval band (last 60 preds if available)\n",
    "try:\n",
    "    if 'q_lo' in globals() and 'q_md' in globals() and 'q_hi' in globals():\n",
    "        mask_q = ~np.isnan(q_lo) & ~np.isnan(q_md) & ~np.isnan(q_hi)\n",
    "        idxs = np.where(mask_q)[0]\n",
    "        if len(idxs) > 0:\n",
    "            tail = idxs[-min(60, len(idxs)):]  # last 60\n",
    "            plt.figure(figsize=(9,4))\n",
    "            plt.fill_between(tail, q_lo[tail], q_hi[tail], alpha=0.3, label=\"90% PI\")\n",
    "            plt.plot(tail, y[tail], label=\"Actual\")\n",
    "            plt.plot(tail, q_md[tail], label=\"Median\")\n",
    "            plt.title(\"Quantile Intervals (last window)\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            p3 = FIGS / \"quantile_intervals.png\"\n",
    "            plt.savefig(p3, dpi=150)\n",
    "            plt.show()\n",
    "            print(\"Saved:\", p3)\n",
    "except Exception as e:\n",
    "    print(\"Quantile plot skipped:\", e)\n",
    "\n",
    "# Feature importance bar (top 20)\n",
    "try:\n",
    "    imp = pd.read_csv(ARTIFACTS / \"feature_importance.csv\").head(20).iloc[::-1]\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.barh(imp[\"feature\"], imp[\"importance\"])\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    p4 = FIGS / \"feature_importance.png\"\n",
    "    plt.savefig(p4, dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", p4)\n",
    "except Exception as e:\n",
    "    print(\"Importance plot skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A8. Outputs Index\n",
    "index = {\n",
    "    \"backtest_overall\": str(ARTIFACTS / \"backtest_overall.csv\"),\n",
    "    \"backtest_monthly\": str(ARTIFACTS / \"backtest_monthly.csv\"),\n",
    "    \"interval_coverage\": str(ARTIFACTS / \"interval_coverage.json\"),\n",
    "    \"policy_simulation\": str(ARTIFACTS / \"policy_simulation.csv\"),\n",
    "    \"event_residuals\": str(ARTIFACTS / \"event_residuals.csv\"),\n",
    "    \"feature_importance\": str(ARTIFACTS / \"feature_importance.csv\"),\n",
    "    \"top_mispredictions\": str(ARTIFACTS / \"top_mispredictions.csv\"),\n",
    "    \"fig_monthly_mae\": str(FIGS / \"monthly_mae.png\"),\n",
    "    \"fig_monthly_smape\": str(FIGS / \"monthly_smape.png\"),\n",
    "    \"fig_quantile_intervals\": str(FIGS / \"quantile_intervals.png\"),\n",
    "    \"fig_feature_importance\": str(FIGS / \"feature_importance.png\"),\n",
    "}\n",
    "print(json.dumps(index, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
