{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79816e9f",
      "metadata": {
        "id": "79816e9f"
      },
      "source": [
        "# Food-Waste Forecasting — MASTER Notebook\n",
        "\n",
        "This single notebook consolidates:\n",
        "- **Baseline (Original)**: Simple train/test split with a **Decision Tree Regressor**, baseline metrics and saved artifacts.\n",
        "- **Upgrades**: Robust preprocessing, feature engineering, Random Forest / (optional) XGBoost.\n",
        "- **Mentor Feedback Additions**: Rolling-origin backtests vs seasonal-naive, quantile regression decision layer with interval coverage, event-effects modeling with hierarchical pooling fallback, SHAP/permutation importances, error analysis, policy simulation, and Overleaf-ready figures.\n",
        "\n",
        "> This merges prior `01_train_decision_tree.ipynb`, `02_model_upgrade.ipynb`, and `03_mentor_feedback.ipynb` functionality into **one** end-to-end notebook.\n",
        "> It does **not** overwrite any existing repo files by default; it only **adds** artifacts/figures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58f96af0",
      "metadata": {
        "id": "58f96af0",
        "outputId": "378a6fa5-2151-4ed2-d9b8-6c1c97cecf76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts -> /content/artifacts\n",
            "Figures   -> /content/reports/figures\n"
          ]
        }
      ],
      "source": [
        "# ## 0. Setup\n",
        "\n",
        "import os, json, warnings, math\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Optional\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    XGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGB_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "ROOT = Path.cwd()\n",
        "ARTIFACTS = ROOT / \"artifacts\"\n",
        "FIGS = ROOT / \"reports\" / \"figures\"\n",
        "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
        "FIGS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Artifacts ->\", ARTIFACTS)\n",
        "print(\"Figures   ->\", FIGS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cf7296b9",
      "metadata": {
        "id": "cf7296b9",
        "outputId": "eba96839-ddb8-456b-e749-ac95210f50bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Data not found. Update CANDIDATE_PATHS above to your CSV.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2171662618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCANDIDATE_PATHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data not found. Update CANDIDATE_PATHS above to your CSV.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Data not found. Update CANDIDATE_PATHS above to your CSV."
          ]
        }
      ],
      "source": [
        "# ## 1. Load Data\n",
        "\n",
        "# Adjust or add your CSV path here if needed.\n",
        "CANDIDATE_PATHS = [\n",
        "    ROOT / \"data\" / \"mess_waste_GIM_500.csv\",\n",
        "    ROOT / \"mess_waste_GIM_500.csv\",\n",
        "    ROOT / \"data\" / \"mess_waste_GIM_daily_exams.csv\",\n",
        "]\n",
        "\n",
        "def find_first(paths):\n",
        "    for p in paths:\n",
        "        if p.exists():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "DATA_PATH = find_first(CANDIDATE_PATHS)\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError(\"Data not found. Update CANDIDATE_PATHS above to your CSV.\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Using data:\", DATA_PATH)\n",
        "display(df.head())\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4cb05c4",
      "metadata": {
        "id": "d4cb05c4"
      },
      "outputs": [],
      "source": [
        "# ## 2. Column Detection (Date / Target / Events)\n",
        "\n",
        "POSSIBLE_TARGETS = [\"food_waste_kg\", \"waste_kg\", \"leftover_kg\", \"wastage\"]\n",
        "DATE_CANDIDATES = [\"date\", \"day\", \"timestamp\", \"dt\"]\n",
        "EVENT_CANDIDATES = [\"event\", \"event_type\", \"special_event\"]\n",
        "\n",
        "target = next((c for c in POSSIBLE_TARGETS if c in df.columns), None)\n",
        "date_col = next((c for c in DATE_CANDIDATES if c in df.columns), None)\n",
        "event_col = next((c for c in EVENT_CANDIDATES if c in df.columns), None)\n",
        "\n",
        "if target is None:\n",
        "    raise KeyError(f\"Please set your target column name to one of: {POSSIBLE_TARGETS}\")\n",
        "\n",
        "if date_col is not None:\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "\n",
        "print(\"Detected -> Target:\", target, \"| Date:\", date_col, \"| Event:\", event_col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a6b7e5",
      "metadata": {
        "id": "f2a6b7e5"
      },
      "source": [
        "# ## 3. Baseline (Original-equivalent): Decision Tree on Simple Split\n",
        "\n",
        "This section preserves the **original spirit** of your first notebook:\n",
        "- Minimal preprocessing\n",
        "- **Train/Test split** (random if no date; last 20% if date exists)\n",
        "- **DecisionTreeRegressor** as baseline\n",
        "- Save baseline metrics to `artifacts/baseline_metrics.json`\n",
        "\n",
        "> This is a **non-destructive** baseline to keep continuity with the original GitHub notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e49300",
      "metadata": {
        "id": "86e49300"
      },
      "outputs": [],
      "source": [
        "# Build features\n",
        "y = df[target].astype(float)\n",
        "X = df.drop(columns=[target])\n",
        "\n",
        "# quick type splits\n",
        "import numpy as np\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# remove raw date from numeric if any\n",
        "if date_col in num_cols: num_cols.remove(date_col)\n",
        "if date_col in cat_cols: cat_cols.remove(date_col)\n",
        "\n",
        "preprocess_min = ColumnTransformer([\n",
        "    (\"num\", \"passthrough\", num_cols),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "])\n",
        "\n",
        "# split\n",
        "if date_col is not None:\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "baseline_pipe = Pipeline([(\"prep\", preprocess_min), (\"dt\", DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
        "baseline_pipe.fit(X_train, y_train)\n",
        "pred_b = baseline_pipe.predict(X_test)\n",
        "\n",
        "def eval_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
        "\n",
        "baseline_metrics = eval_metrics(y_test, pred_b)\n",
        "\n",
        "with open(ARTIFACTS / \"baseline_metrics.json\", \"w\") as f:\n",
        "    json.dump(baseline_metrics, f, indent=2)\n",
        "\n",
        "print(\"Baseline metrics:\", baseline_metrics)\n",
        "print(\"Saved ->\", ARTIFACTS / \"baseline_metrics.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f985ca",
      "metadata": {
        "id": "92f985ca"
      },
      "source": [
        "# ## 4. Upgrades: Robust Preprocessing + Stronger Models\n",
        "\n",
        "We extend with:\n",
        "- `ColumnTransformer` (scaler + one-hot)\n",
        "- Time features if a date exists (DOW, month, weekend; rolling means)\n",
        "- Models: Decision Tree, Random Forest, (optional) XGBoost\n",
        "- Save metrics to `artifacts/metrics_upgrade.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a00d31",
      "metadata": {
        "id": "03a00d31"
      },
      "outputs": [],
      "source": [
        "# Lightweight time features\n",
        "df_fe = df.copy()\n",
        "if date_col is not None:\n",
        "    d = df_fe[date_col].dt\n",
        "    df_fe[\"dow\"] = d.dayofweek\n",
        "    df_fe[\"month\"] = d.month\n",
        "    df_fe[\"is_weekend\"] = (d.dayofweek >= 5).astype(int)\n",
        "    for win in (3, 7):\n",
        "        df_fe[f\"roll_mean_{win}\"] = df_fe[target].rolling(win, min_periods=1).mean().shift(1)\n",
        "        df_fe[f\"roll_std_{win}\"]  = df_fe[target].rolling(win, min_periods=1).std().shift(1)\n",
        "\n",
        "y2 = df_fe[target].astype(float)\n",
        "X2 = df_fe.drop(columns=[target])\n",
        "\n",
        "num_cols2 = X2.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols2 = X2.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "if date_col in num_cols2: num_cols2.remove(date_col)\n",
        "if date_col in cat_cols2: cat_cols2.remove(date_col)\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(with_mean=False), num_cols2),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols2),\n",
        "])\n",
        "\n",
        "# time-aware split if date exists\n",
        "if date_col is not None:\n",
        "    split_idx = int(len(df_fe) * 0.8)\n",
        "    Xtr, Xte = X2.iloc[:split_idx], X2.iloc[split_idx:]\n",
        "    ytr, yte = y2.iloc[:split_idx], y2.iloc[split_idx:]\n",
        "else:\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X2, y2, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "models = {\n",
        "    \"decision_tree\": DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
        "    \"random_forest\": RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "}\n",
        "if XGB_AVAILABLE:\n",
        "    models[\"xgb\"] = XGBRegressor(\n",
        "        n_estimators=600, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
        "        random_state=RANDOM_STATE, objective=\"reg:squarederror\"\n",
        "    )\n",
        "\n",
        "metrics_upgrade = {}\n",
        "fitted = {}\n",
        "for name, est in models.items():\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", est)])\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    fitted[name] = pipe\n",
        "    yhat = pipe.predict(Xte)\n",
        "    metrics_upgrade[name] = eval_metrics(yte, yhat)\n",
        "\n",
        "with open(ARTIFACTS / \"metrics_upgrade.json\", \"w\") as f:\n",
        "    json.dump(metrics_upgrade, f, indent=2)\n",
        "\n",
        "print(\"Saved ->\", ARTIFACTS / \"metrics_upgrade.json\")\n",
        "metrics_upgrade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac6ab15",
      "metadata": {
        "id": "4ac6ab15"
      },
      "source": [
        "# ## 5. Mentor Feedback: Rolling-Origin vs Seasonal-Naive, Quantiles, Events, Explainability\n",
        "\n",
        "This block implements the mentor’s requests while keeping prior results intact.\n",
        "Outputs are saved to `artifacts/` and `reports/figures/` for Overleaf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3951363",
      "metadata": {
        "id": "f3951363"
      },
      "outputs": [],
      "source": [
        "# ----- Helpers -----\n",
        "def smape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    diff = np.abs(y_true - y_pred)\n",
        "    out = np.zeros_like(denom)\n",
        "    mask = denom != 0\n",
        "    out[mask] = diff[mask] / denom[mask]\n",
        "    return 100.0 * np.mean(out)\n",
        "\n",
        "def seasonal_naive(y_series, period=7):\n",
        "    y = np.asarray(y_series)\n",
        "    y_hat = np.full_like(y, np.nan, dtype=float)\n",
        "    for t in range(period, len(y)):\n",
        "        y_hat[t] = y[t - period]\n",
        "    return y_hat\n",
        "\n",
        "def interval_coverage(y_true, lo, hi):\n",
        "    y = np.asarray(y_true)\n",
        "    return float(np.mean((y >= lo) & (y <= hi)))\n",
        "\n",
        "def waste_shortage(prepared, actual):\n",
        "    prepared = np.asarray(prepared)\n",
        "    actual = np.asarray(actual)\n",
        "    waste = np.maximum(0.0, prepared - actual)\n",
        "    shortage = np.maximum(0.0, actual - prepared)\n",
        "    return waste, shortage\n",
        "\n",
        "# ----- Backtest RF vs Seasonal-Naive (expanding window, H=1) -----\n",
        "rf_bt = RandomForestRegressor(n_estimators=400, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "rf_bt_pipe = Pipeline([(\"prep\", preprocessor), (\"rf\", rf_bt)])\n",
        "\n",
        "y_array = y2.values.astype(float)\n",
        "y_naive = seasonal_naive(y_array, period=7)\n",
        "preds_rf = np.full_like(y_array, np.nan, dtype=float)\n",
        "\n",
        "start = 14  # 2-week warmup\n",
        "for t in range(start, len(df_fe)-1):\n",
        "    train_idx = slice(0, t+1)\n",
        "    test_idx = t+1\n",
        "    rf_bt_pipe.fit(X2.iloc[train_idx], y_array[train_idx])\n",
        "    preds_rf[test_idx] = rf_bt_pipe.predict(X2.iloc[[test_idx]])[0]\n",
        "\n",
        "mask = ~np.isnan(preds_rf) & ~np.isnan(y_naive)\n",
        "overall = {\n",
        "    \"RF_MAE\": float(mean_absolute_error(y_array[mask], preds_rf[mask])),\n",
        "    \"RF_SMAPE\": float(smape(y_array[mask], preds_rf[mask])),\n",
        "    \"Naive_MAE\": float(mean_absolute_error(y_array[mask], y_naive[mask])),\n",
        "    \"Naive_SMAPE\": float(smape(y_array[mask], y_naive[mask])),\n",
        "}\n",
        "overall[\"Improvement_MAE_%\"] = 100.0 * (1 - overall[\"RF_MAE\"] / overall[\"Naive_MAE\"])\n",
        "overall[\"Improvement_SMAPE_%\"] = 100.0 * (1 - overall[\"RF_SMAPE\"] / overall[\"Naive_SMAPE\"])\n",
        "\n",
        "months = df_fe[date_col].dt.to_period(\"M\").astype(str).values if date_col is not None else np.array([\"ALL\"]*len(df_fe))\n",
        "bt_df = pd.DataFrame({\"month\": months, \"y\": y_array, \"rf\": preds_rf, \"naive\": y_naive}).dropna()\n",
        "\n",
        "monthly_rows = []\n",
        "for m, g in bt_df.groupby(\"month\"):\n",
        "    monthly_rows.append({\n",
        "        \"month\": m,\n",
        "        \"RF_MAE\": float(mean_absolute_error(g[\"y\"], g[\"rf\"])),\n",
        "        \"RF_SMAPE\": float(smape(g[\"y\"], g[\"rf\"])),\n",
        "        \"Naive_MAE\": float(mean_absolute_error(g[\"y\"], g[\"naive\"])),\n",
        "        \"Naive_SMAPE\": float(smape(g[\"y\"], g[\"naive\"])),\n",
        "    })\n",
        "monthly_df = pd.DataFrame(monthly_rows).sort_values(\"month\")\n",
        "\n",
        "pd.DataFrame([overall]).to_csv(ARTIFACTS / \"backtest_overall.csv\", index=False)\n",
        "monthly_df.to_csv(ARTIFACTS / \"backtest_monthly.csv\", index=False)\n",
        "\n",
        "print(\"Saved ->\", ARTIFACTS / \"backtest_overall.csv\")\n",
        "print(\"Saved ->\", ARTIFACTS / \"backtest_monthly.csv\")\n",
        "\n",
        "# ----- Quantile Regression Decision Layer (0.1/0.5/0.9) -----\n",
        "q_models = {}\n",
        "for q in [0.1, 0.5, 0.9]:\n",
        "    q_models[q] = Pipeline([\n",
        "        (\"prep\", preprocessor),\n",
        "        (\"gbr\", GradientBoostingRegressor(loss=\"quantile\", alpha=q, random_state=RANDOM_STATE, n_estimators=400, max_depth=3))\n",
        "    ])\n",
        "\n",
        "q_lo = np.full_like(y_array, np.nan, dtype=float)\n",
        "q_md = np.full_like(y_array, np.nan, dtype=float)\n",
        "q_hi = np.full_like(y_array, np.nan, dtype=float)\n",
        "\n",
        "for t in range(start, len(df_fe)-1):\n",
        "    train_idx = slice(0, t+1)\n",
        "    test_idx = t+1\n",
        "    for q in [0.1, 0.5, 0.9]:\n",
        "        q_models[q].fit(X2.iloc[train_idx], y_array[train_idx])\n",
        "    q_lo[test_idx] = q_models[0.1].predict(X2.iloc[[test_idx]])[0]\n",
        "    q_md[test_idx] = q_models[0.5].predict(X2.iloc[[test_idx]])[0]\n",
        "    q_hi[test_idx] = q_models[0.9].predict(X2.iloc[[test_idx]])[0]\n",
        "\n",
        "mask_q = ~np.isnan(q_lo) & ~np.isnan(q_md) & ~np.isnan(q_hi)\n",
        "coverage90 = interval_coverage(y_array[mask_q], q_lo[mask_q], q_hi[mask_q])\n",
        "with open(ARTIFACTS / \"interval_coverage.json\", \"w\") as f:\n",
        "    json.dump({\"coverage90\": coverage90}, f, indent=2)\n",
        "print(\"Saved ->\", ARTIFACTS / \"interval_coverage.json\")\n",
        "\n",
        "# Policy simulation over lambda\n",
        "lams = np.linspace(0, 1, 6)\n",
        "policy = []\n",
        "for lam in lams:\n",
        "    prepared = q_md.copy()\n",
        "    prepared[mask_q] = q_md[mask_q] + lam * (q_hi[mask_q] - q_md[mask_q])\n",
        "    waste, shortage = waste_shortage(prepared[mask_q], y_array[mask_q])\n",
        "    policy.append({\n",
        "        \"lambda\": float(lam),\n",
        "        \"avg_waste\": float(np.mean(waste)),\n",
        "        \"avg_shortage\": float(np.mean(shortage)),\n",
        "        \"efficiency_%\": float(100.0 * (1 - (np.mean(waste) / (np.mean(y_array[mask_q]) + 1e-8))))\n",
        "    })\n",
        "policy_df = pd.DataFrame(policy).sort_values(\"lambda\")\n",
        "policy_df.to_csv(ARTIFACTS / \"policy_simulation.csv\", index=False)\n",
        "print(\"Saved ->\", ARTIFACTS / \"policy_simulation.csv\")\n",
        "\n",
        "# ----- Event Effects + Hierarchical Pooling Fallback -----\n",
        "EVENT_CANDIDATES = [\"event\", \"event_type\", \"special_event\"]\n",
        "event_use = next((c for c in EVENT_CANDIDATES if c in df_fe.columns), None)\n",
        "if event_use is None:\n",
        "    df_fe[\"event_synth\"] = \"Normal\"\n",
        "    event_use = \"event_synth\"\n",
        "\n",
        "bt_ev = pd.DataFrame({\n",
        "    \"date\": df_fe[date_col] if date_col is not None else pd.RangeIndex(len(df_fe)),\n",
        "    \"y\": y_array,\n",
        "    \"rf\": preds_rf,\n",
        "    \"event\": df_fe[event_use]\n",
        "}).dropna()\n",
        "\n",
        "bt_ev[\"resid\"] = bt_ev[\"y\"] - bt_ev[\"rf\"]\n",
        "evt_stats = bt_ev.groupby(\"event\")[\"resid\"].agg([\"mean\",\"std\",\"count\"]).reset_index().rename(columns={\"mean\":\"resid_mean\",\"std\":\"resid_std\",\"count\":\"n\"})\n",
        "evt_stats[\"global_mean\"] = bt_ev[\"resid\"].mean()\n",
        "k = max(1.0, evt_stats[\"n\"].median())\n",
        "evt_stats[\"shrink_mean\"] = (evt_stats[\"n\"]/(evt_stats[\"n\"]+k))*evt_stats[\"resid_mean\"] + (k/(evt_stats[\"n\"]+k))*evt_stats[\"global_mean\"]\n",
        "evt_stats.to_csv(ARTIFACTS / \"event_residuals.csv\", index=False)\n",
        "print(\"Saved ->\", ARTIFACTS / \"event_residuals.csv\")\n",
        "\n",
        "# ----- Explainability: SHAP (if available) or permutation -----\n",
        "rf_full = Pipeline([(\"prep\", preprocessor), (\"rf\", RandomForestRegressor(n_estimators=600, random_state=RANDOM_STATE, n_jobs=-1))])\n",
        "rf_full.fit(X2, y2)\n",
        "\n",
        "feat_names = []\n",
        "try:\n",
        "    feat_names += list(preprocessor.transformers_[0][2])  # numeric\n",
        "    ohe = preprocessor.named_transformers_[\"cat\"]\n",
        "    base = preprocessor.transformers_[1][2]\n",
        "    feat_names += ohe.get_feature_names_out(base).tolist()\n",
        "except Exception:\n",
        "    feat_names = [f\"f{i}\" for i in range(rf_full.named_steps[\"rf\"].n_features_in_)]\n",
        "\n",
        "importance_df = None\n",
        "if SHAP_AVAILABLE:\n",
        "    try:\n",
        "        explainer = shap.TreeExplainer(rf_full.named_steps[\"rf\"])\n",
        "        X_tr = preprocessor.fit_transform(X2)\n",
        "        idx = np.linspace(0, X_tr.shape[0]-1, min(500, X_tr.shape[0])).astype(int)\n",
        "        sv = explainer.shap_values(X_tr[idx])\n",
        "        shap_abs = np.mean(np.abs(sv), axis=0)\n",
        "        importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": shap_abs}).sort_values(\"importance\", ascending=False)\n",
        "    except Exception as e:\n",
        "        print(\"SHAP error, using permutation:\", e)\n",
        "\n",
        "if importance_df is None:\n",
        "    perm = permutation_importance(rf_full, X2, y2, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": perm.importances_mean}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "importance_df.to_csv(ARTIFACTS / \"feature_importance.csv\", index=False)\n",
        "print(\"Saved ->\", ARTIFACTS / \"feature_importance.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d231e11",
      "metadata": {
        "id": "5d231e11"
      },
      "outputs": [],
      "source": [
        "# ## 6. Overleaf-Ready Plots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Monthly MAE / SMAPE\n",
        "if (ARTIFACTS / \"backtest_monthly.csv\").exists():\n",
        "    monthly_df = pd.read_csv(ARTIFACTS / \"backtest_monthly.csv\")\n",
        "    if not monthly_df.empty:\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.plot(monthly_df[\"month\"], monthly_df[\"RF_MAE\"], marker='o', label=\"RF MAE\")\n",
        "        plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_MAE\"], marker='o', label=\"Naive MAE\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title(\"Monthly MAE — Rolling-Origin Backtest\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        p1 = FIGS / \"monthly_mae.png\"\n",
        "        plt.savefig(p1, dpi=150)\n",
        "        plt.show()\n",
        "        print(\"Saved ->\", p1)\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.plot(monthly_df[\"month\"], monthly_df[\"RF_SMAPE\"], marker='o', label=\"RF SMAPE\")\n",
        "        plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_SMAPE\"], marker='o', label=\"Naive SMAPE\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title(\"Monthly SMAPE — Rolling-Origin Backtest\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        p2 = FIGS / \"monthly_smape.png\"\n",
        "        plt.savefig(p2, dpi=150)\n",
        "        plt.show()\n",
        "        print(\"Saved ->\", p2)\n",
        "\n",
        "# Quantile interval band (last 60 predictions if exist)\n",
        "import numpy as np, pandas as pd\n",
        "if (ARTIFACTS / \"interval_coverage.json\").exists():\n",
        "    try:\n",
        "        # Load arrays from memory section if still defined; otherwise skip\n",
        "        if 'q_lo' in globals() and 'q_md' in globals() and 'q_hi' in globals() and 'y_array' in globals():\n",
        "            mask_q = ~np.isnan(q_lo) & ~np.isnan(q_md) & ~np.isnan(q_hi)\n",
        "            idxs = np.where(mask_q)[0]\n",
        "            if len(idxs) > 0:\n",
        "                tail = idxs[-min(60, len(idxs)):]  # last 60\n",
        "                plt.figure(figsize=(9,4))\n",
        "                plt.fill_between(tail, q_lo[tail], q_hi[tail], alpha=0.3, label=\"90% PI\")\n",
        "                plt.plot(tail, y_array[tail], label=\"Actual\")\n",
        "                plt.plot(tail, q_md[tail], label=\"Median\")\n",
        "                plt.title(\"Quantile Intervals (last window)\")\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                p3 = FIGS / \"quantile_intervals.png\"\n",
        "                plt.savefig(p3, dpi=150)\n",
        "                plt.show()\n",
        "                print(\"Saved ->\", p3)\n",
        "    except Exception as e:\n",
        "        print(\"Quantile plot skipped:\", e)\n",
        "\n",
        "# Feature importances\n",
        "import pandas as pd\n",
        "imp_csv = ARTIFACTS / \"feature_importance.csv\"\n",
        "if imp_csv.exists():\n",
        "    imp = pd.read_csv(imp_csv).head(20).iloc[::-1]\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.barh(imp[\"feature\"], imp[\"importance\"])\n",
        "    plt.title(\"Top Feature Importances\")\n",
        "    plt.tight_layout()\n",
        "    p4 = FIGS / \"feature_importance.png\"\n",
        "    plt.savefig(p4, dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Saved ->\", p4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3ebb0b",
      "metadata": {
        "id": "5b3ebb0b"
      },
      "outputs": [],
      "source": [
        "# ## 7. Outputs Index (Artifacts & Figures)\n",
        "\n",
        "index = {\n",
        "    \"baseline_metrics\": str(ARTIFACTS / \"baseline_metrics.json\"),\n",
        "    \"upgrade_metrics\": str(ARTIFACTS / \"metrics_upgrade.json\"),\n",
        "    \"backtest_overall\": str(ARTIFACTS / \"backtest_overall.csv\"),\n",
        "    \"backtest_monthly\": str(ARTIFACTS / \"backtest_monthly.csv\"),\n",
        "    \"interval_coverage\": str(ARTIFACTS / \"interval_coverage.json\"),\n",
        "    \"policy_simulation\": str(ARTIFACTS / \"policy_simulation.csv\"),\n",
        "    \"event_residuals\": str(ARTIFACTS / \"event_residuals.csv\"),\n",
        "    \"feature_importance\": str(ARTIFACTS / \"feature_importance.csv\"),\n",
        "    \"top_mispredictions\": str(ARTIFACTS / \"top_mispredictions.csv\"),\n",
        "    \"fig_monthly_mae\": str(FIGS / \"monthly_mae.png\"),\n",
        "    \"fig_monthly_smape\": str(FIGS / \"monthly_smape.png\"),\n",
        "    \"fig_quantile_intervals\": str(FIGS / \"quantile_intervals.png\"),\n",
        "    \"fig_feature_importance\": str(FIGS / \"feature_importance.png\"),\n",
        "}\n",
        "\n",
        "import json\n",
        "print(json.dumps(index, indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}