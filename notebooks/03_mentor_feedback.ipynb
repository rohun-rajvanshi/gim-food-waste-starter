{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bd979a",
   "metadata": {},
   "source": [
    "# Mentor Feedback Implementation — GIM Mess Food-Waste Forecasting\n",
    "\n",
    "This notebook adds the exact items requested by your mentor **without altering your baseline**:\n",
    "1. Rolling-origin (expanding window) backtests vs seasonal-naive; report **MAE/SMAPE** (overall and by month).\n",
    "2. **Decision layer** with prediction intervals via quantile regression (0.1/0.5/0.9); **interval coverage**.\n",
    "3. **Event effects** modeling + **fallback for unseen events** via hierarchical pooling; error analysis by event type.\n",
    "4. **Explainability**: SHAP (if installed) or permutation importance → actionable kitchen levers.\n",
    "5. **Imputation** + **cold-start** strategy (weekday-grouped impute + hierarchical pooling).\n",
    "\n",
    "Artifacts are saved to `artifacts/` and `reports/figures/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 0. Setup\n",
    "import os, json, math, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except Exception:\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "RNG = np.random.RandomState(42)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "FIGS = ROOT / \"reports\" / \"figures\"\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Artifacts:\", ARTIFACTS)\n",
    "print(\"Figures:\", FIGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Load Data and Identify Columns\n",
    "\n",
    "# Try common locations; edit if needed\n",
    "CANDIDATES = [\n",
    "    ROOT / \"data\" / \"mess_waste_GIM_500.csv\",\n",
    "    ROOT / \"mess_waste_GIM_500.csv\",\n",
    "    ROOT / \"data\" / \"mess_waste_GIM_daily_exams.csv\",\n",
    "]\n",
    "def pick(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Please update data path\")\n",
    "\n",
    "DATA_PATH = pick(CANDIDATES)\n",
    "print(\"Using data:\", DATA_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Infer date, target, event columns\n",
    "DATE_CAND = [\"date\", \"day\", \"timestamp\", \"dt\"]\n",
    "TARGET_CAND = [\"food_waste_kg\", \"waste_kg\", \"leftover_kg\", \"wastage\"]\n",
    "EVENT_CAND = [\"event\", \"event_type\", \"special_event\"]\n",
    "\n",
    "date_col = next((c for c in DATE_CAND if c in df.columns), None)\n",
    "target = next((c for c in TARGET_CAND if c in df.columns), None)\n",
    "event_col = next((c for c in EVENT_CAND if c in df.columns), None)\n",
    "\n",
    "if date_col is None: \n",
    "    raise KeyError(\"No date column found. Add one of: \" + \",\".join(DATE_CAND))\n",
    "if target is None:\n",
    "    raise KeyError(\"No target column found. Add one of: \" + \",\".join(TARGET_CAND))\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "# Basic engineered time features\n",
    "df[\"dow\"] = df[date_col].dt.dayofweek\n",
    "df[\"month\"] = df[date_col].dt.month\n",
    "df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "# Weekday-grouped imputation example (for attendance/menu counts etc.)\n",
    "# Fill numeric NaNs with weekday means then global mean\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != target]\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df.groupby(\"dow\")[c].transform(lambda s: s.fillna(s.mean()))\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# Categorical impute: fill missing with 'Unknown'\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna(\"Unknown\")\n",
    "    \n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Target:\", target, \"| Date:\", date_col, \"| Event:\", event_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997affe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Metrics, Baselines, and Utilities\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    # Avoid division by zero\n",
    "    mask = denom == 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[~mask] = diff[~mask] / denom[~mask]\n",
    "    return 100.0 * np.mean(out)\n",
    "\n",
    "def seasonal_naive(y_series, period=7):\n",
    "    # y_hat[t] = y[t - period]\n",
    "    y = np.asarray(y_series)\n",
    "    y_hat = np.empty_like(y, dtype=float)\n",
    "    y_hat[:] = np.nan\n",
    "    for t in range(period, len(y)):\n",
    "        y_hat[t] = y[t - period]\n",
    "    return y_hat\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def month_name(dt):\n",
    "    return dt.strftime(\"%Y-%m\")\n",
    "\n",
    "def interval_coverage(y_true, lo, hi):\n",
    "    y = np.asarray(y_true)\n",
    "    return np.mean((y >= lo) & (y <= hi))\n",
    "\n",
    "def waste_shortage(prepared, actual):\n",
    "    prepared = np.asarray(prepared)\n",
    "    actual = np.asarray(actual)\n",
    "    waste = np.maximum(0.0, prepared - actual)\n",
    "    shortage = np.maximum(0.0, actual - prepared)\n",
    "    return waste, shortage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda94e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Preprocessor and Feature Matrix\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Do not leak target\n",
    "if target in numeric_cols: numeric_cols.remove(target)\n",
    "\n",
    "# Do not feed raw date; already encoded via features\n",
    "if date_col in numeric_cols: numeric_cols.remove(date_col)\n",
    "if date_col in categorical_cols: categorical_cols.remove(date_col)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(with_mean=False), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "])\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ee79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Rolling-Origin Backtest (Expanding Window, H=1)\n",
    "\n",
    "# Models\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400, max_depth=None, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "rf_pipe = Pipeline([(\"prep\", preprocessor), (\"rf\", rf)])\n",
    "\n",
    "# Seasonal naive requires only y history\n",
    "y_naive = seasonal_naive(y, period=7)\n",
    "\n",
    "# Rolling-origin expanding window\n",
    "preds_rf = np.full_like(y, fill_value=np.nan, dtype=float)\n",
    "months = df[date_col].dt.to_period(\"M\").astype(str).values\n",
    "\n",
    "start = 14  # minimal warmup (2 weeks) — adjust if needed\n",
    "for t in range(start, len(df)-1):\n",
    "    train_idx = slice(0, t+1)   # up to current t\n",
    "    test_idx = t+1              # predict next day\n",
    "    \n",
    "    rf_pipe.fit(X.iloc[train_idx], y[train_idx])\n",
    "    preds_rf[test_idx] = rf_pipe.predict(X.iloc[[test_idx]])[0]\n",
    "\n",
    "# Metrics overall (align indices where both preds exist)\n",
    "mask = ~np.isnan(preds_rf) & ~np.isnan(y_naive)\n",
    "res_overall = {\n",
    "    \"RF_MAE\": mae(y[mask], preds_rf[mask]),\n",
    "    \"RF_SMAPE\": smape(y[mask], preds_rf[mask]),\n",
    "    \"Naive_MAE\": mae(y[mask], y_naive[mask]),\n",
    "    \"Naive_SMAPE\": smape(y[mask], y_naive[mask]),\n",
    "    \"Improvement_MAE_%\": 100.0 * (1 - mae(y[mask], preds_rf[mask]) / mae(y[mask], y_naive[mask])),\n",
    "    \"Improvement_SMAPE_%\": 100.0 * (1 - smape(y[mask], preds_rf[mask]) / smape(y[mask], y_naive[mask])),\n",
    "}\n",
    "print(\"Overall backtest:\", res_overall)\n",
    "\n",
    "# By month\n",
    "df_bt = pd.DataFrame({\n",
    "    \"month\": months,\n",
    "    \"y\": y,\n",
    "    \"rf\": preds_rf,\n",
    "    \"naive\": y_naive\n",
    "})\n",
    "df_bt = df_bt.dropna()\n",
    "\n",
    "monthly = []\n",
    "for m, g in df_bt.groupby(\"month\"):\n",
    "    monthly.append({\n",
    "        \"month\": m,\n",
    "        \"RF_MAE\": mae(g[\"y\"], g[\"rf\"]),\n",
    "        \"RF_SMAPE\": smape(g[\"y\"], g[\"rf\"]),\n",
    "        \"Naive_MAE\": mae(g[\"y\"], g[\"naive\"]),\n",
    "        \"Naive_SMAPE\": smape(g[\"y\"], g[\"naive\"]),\n",
    "    })\n",
    "monthly_df = pd.DataFrame(monthly).sort_values(\"month\")\n",
    "\n",
    "# Save\n",
    "pd.DataFrame([res_overall]).to_csv(ARTIFACTS / \"backtest_overall.csv\", index=False)\n",
    "monthly_df.to_csv(ARTIFACTS / \"backtest_monthly.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"backtest_overall.csv\")\n",
    "print(\"Saved:\", ARTIFACTS / \"backtest_monthly.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. Quantile Regression Decision Layer (0.1, 0.5, 0.9)\n",
    "\n",
    "# We'll use GradientBoostingRegressor with quantile loss to avoid extra dependencies.\n",
    "q_models = {}\n",
    "for q in [0.1, 0.5, 0.9]:\n",
    "    q_models[q] = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"gbr\", GradientBoostingRegressor(loss=\"quantile\", alpha=q, random_state=42, n_estimators=400, max_depth=3))\n",
    "    ])\n",
    "\n",
    "q_lo = np.full_like(y, np.nan, dtype=float)\n",
    "q_md = np.full_like(y, np.nan, dtype=float)\n",
    "q_hi = np.full_like(y, np.nan, dtype=float)\n",
    "\n",
    "for t in range(start, len(df)-1):\n",
    "    train_idx = slice(0, t+1)\n",
    "    test_idx = t+1\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        q_models[q].fit(X.iloc[train_idx], y[train_idx])\n",
    "    q_lo[test_idx] = q_models[0.1].predict(X.iloc[[test_idx]])[0]\n",
    "    q_md[test_idx] = q_models[0.5].predict(X.iloc[[test_idx]])[0]\n",
    "    q_hi[test_idx] = q_models[0.9].predict(X.iloc[[test_idx]])[0]\n",
    "\n",
    "mask_q = ~np.isnan(q_lo) & ~np.isnan(q_md) & ~np.isnan(q_hi)\n",
    "coverage90 = interval_coverage(y[mask_q], q_lo[mask_q], q_hi[mask_q])\n",
    "print(\"90% interval coverage:\", coverage90)\n",
    "\n",
    "# Policy simulation: prepared = q_md + lambda*(q_hi - q_md)\n",
    "lambdas = np.linspace(0, 1, 6)\n",
    "policy_rows = []\n",
    "for lam in lambdas:\n",
    "    prepared = q_md.copy()\n",
    "    prepared[mask_q] = q_md[mask_q] + lam * (q_hi[mask_q] - q_md[mask_q])\n",
    "    w, s = waste_shortage(prepared[mask_q], y[mask_q])\n",
    "    policy_rows.append({\n",
    "        \"lambda\": lam,\n",
    "        \"avg_waste\": np.mean(w),\n",
    "        \"avg_shortage\": np.mean(s),\n",
    "        \"efficiency_%\": 100.0 * (1 - (np.mean(w) / (np.mean(y[mask_q]) + 1e-8)))  # heuristic\n",
    "    })\n",
    "policy_df = pd.DataFrame(policy_rows)\n",
    "policy_df.to_csv(ARTIFACTS / \"policy_simulation.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"policy_simulation.csv\")\n",
    "\n",
    "# Save interval coverage\n",
    "with open(ARTIFACTS / \"interval_coverage.json\", \"w\") as f:\n",
    "    json.dump({\"coverage90\": float(coverage90)}, f, indent=2)\n",
    "print(\"Saved:\", ARTIFACTS / \"interval_coverage.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Event Effects Modeling + Hierarchical Pooling Fallback\n",
    "\n",
    "# We assume event_col is categorical like ['Normal','Fest','Exam', ...].\n",
    "# We'll compute residuals by event type from the RF rolling forecast,\n",
    "# and implement a shrinkage estimate for unseen events.\n",
    "\n",
    "if event_col is None:\n",
    "    print(\"No explicit event column found; synthesizing 'Normal' as placeholder.\")\n",
    "    df[\"event_synth\"] = \"Normal\"\n",
    "    event_col_use = \"event_synth\"\n",
    "else:\n",
    "    event_col_use = event_col\n",
    "\n",
    "df_bt2 = pd.DataFrame({\n",
    "    \"date\": df[date_col],\n",
    "    \"y\": y,\n",
    "    \"rf\": preds_rf,\n",
    "    \"event\": df[event_col_use]\n",
    "}).dropna()\n",
    "\n",
    "df_bt2[\"resid\"] = df_bt2[\"y\"] - df_bt2[\"rf\"]\n",
    "\n",
    "# Per-event residual summaries\n",
    "evt_stats = df_bt2.groupby(\"event\")[\"resid\"].agg([\"mean\",\"std\",\"count\"]).reset_index().rename(columns={\"mean\":\"resid_mean\",\"std\":\"resid_std\",\"count\":\"n\"})\n",
    "evt_stats[\"global_mean\"] = df_bt2[\"resid\"].mean()\n",
    "evt_stats[\"global_var\"] = df_bt2[\"resid\"].var()\n",
    "\n",
    "# Empirical Bayes style shrinkage: m = (n/(n+k))*event_mean + (k/(n+k))*global_mean\n",
    "k = max(1.0, evt_stats[\"n\"].median())  # pseudo-count\n",
    "evt_stats[\"shrink_mean\"] = (evt_stats[\"n\"]/(evt_stats[\"n\"]+k))*evt_stats[\"resid_mean\"] + (k/(evt_stats[\"n\"]+k))*evt_stats[\"global_mean\"]\n",
    "\n",
    "evt_stats.to_csv(ARTIFACTS / \"event_residuals.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"event_residuals.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Explainability → Actionable Levers (SHAP if available else permutation)\n",
    "\n",
    "# Fit final RF on full data to extract importances\n",
    "rf_final = Pipeline([(\"prep\", preprocessor), (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))])\n",
    "rf_final.fit(X, y)\n",
    "\n",
    "feat_names = []\n",
    "try:\n",
    "    # numeric first\n",
    "    feat_names += list(preprocessor.transformers_[0][2])\n",
    "    # then onehot names\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "    cat_base = preprocessor.transformers_[1][2]\n",
    "    ohe_names = ohe.get_feature_names_out(cat_base).tolist()\n",
    "    feat_names += ohe_names\n",
    "except Exception:\n",
    "    feat_names = [f\"f{i}\" for i in range(rf_final.named_steps[\"rf\"].n_features_in_)]\n",
    "\n",
    "importance_df = None\n",
    "if SHAP_AVAILABLE:\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(rf_final.named_steps[\"rf\"])\n",
    "        X_trans = preprocessor.fit_transform(X)  # ensure fitted names align\n",
    "        # Use a small sample for speed\n",
    "        idx = np.linspace(0, X_trans.shape[0]-1, min(500, X_trans.shape[0])).astype(int)\n",
    "        shap_values = explainer.shap_values(X_trans[idx])\n",
    "        shap_abs_mean = np.mean(np.abs(shap_values), axis=0)\n",
    "        importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": shap_abs_mean}).sort_values(\"importance\", ascending=False)\n",
    "    except Exception as e:\n",
    "        print(\"SHAP failed, falling back to permutation:\", e)\n",
    "\n",
    "if importance_df is None:\n",
    "    perm = permutation_importance(rf_final, X, y, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "    importance_df = pd.DataFrame({\"feature\": feat_names, \"importance\": perm.importances_mean}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "importance_df.to_csv(ARTIFACTS / \"feature_importance.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"feature_importance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 8. Error Analysis: Top Mispredictions with Root Causes\n",
    "\n",
    "# Use RF rolling predictions (preds_rf) for realistic errors\n",
    "bt_err = pd.DataFrame({\n",
    "    \"date\": df[date_col],\n",
    "    \"y\": y,\n",
    "    \"rf\": preds_rf\n",
    "})\n",
    "bt_err[\"abs_err\"] = np.abs(bt_err[\"y\"] - bt_err[\"rf\"])\n",
    "bt_err = bt_err.dropna().sort_values(\"abs_err\", ascending=False)\n",
    "\n",
    "TOPK = 15\n",
    "context_cols = [c for c in df.columns if c not in [target]]\n",
    "top_err = bt_err.head(TOPK).merge(df[context_cols], left_index=True, right_index=True, how=\"left\")\n",
    "top_err.to_csv(ARTIFACTS / \"top_mispredictions.csv\", index=False)\n",
    "print(\"Saved:\", ARTIFACTS / \"top_mispredictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf455a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 9. Quick Plots (saved for Overleaf)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Monthly MAE/SMAPE bar\n",
    "fig1 = plt.figure(figsize=(8,4))\n",
    "plt.plot(monthly_df[\"month\"], monthly_df[\"RF_MAE\"], marker='o', label=\"RF MAE\")\n",
    "plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_MAE\"], marker='o', label=\"Naive MAE\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Monthly MAE — Rolling-Origin Backtest\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "p1 = FIGS / \"monthly_mae.png\"\n",
    "plt.savefig(p1, dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", p1)\n",
    "\n",
    "fig2 = plt.figure(figsize=(8,4))\n",
    "plt.plot(monthly_df[\"month\"], monthly_df[\"RF_SMAPE\"], marker='o', label=\"RF SMAPE\")\n",
    "plt.plot(monthly_df[\"month\"], monthly_df[\"Naive_SMAPE\"], marker='o', label=\"Naive SMAPE\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Monthly SMAPE — Rolling-Origin Backtest\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "p2 = FIGS / \"monthly_smape.png\"\n",
    "plt.savefig(p2, dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", p2)\n",
    "\n",
    "# Quantile interval band (last 60 days if available)\n",
    "fig3 = plt.figure(figsize=(9,4))\n",
    "mask_plot = mask_q.copy()\n",
    "# focus last window\n",
    "idxs = np.where(mask_plot)[0]\n",
    "if len(idxs) > 0:\n",
    "    tail = idxs[-min(60, len(idxs)):]  # last 60 predictions\n",
    "    plt.fill_between(tail, q_lo[tail], q_hi[tail], alpha=0.3, label=\"90% PI\")\n",
    "    plt.plot(tail, y[tail], label=\"Actual\")\n",
    "    plt.plot(tail, q_md[tail], label=\"Median forecast\")\n",
    "    plt.title(\"Quantile Intervals (last window)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p3 = FIGS / \"quantile_intervals.png\"\n",
    "    plt.savefig(p3, dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", p3)\n",
    "\n",
    "# Feature importance top-20\n",
    "top_imp = importance_df.head(20).iloc[::-1]\n",
    "fig4 = plt.figure(figsize=(7,6))\n",
    "plt.barh(top_imp[\"feature\"], top_imp[\"importance\"])\n",
    "plt.title(\"Top Feature Importances\")\n",
    "plt.tight_layout()\n",
    "p4 = FIGS / \"feature_importance.png\"\n",
    "plt.savefig(p4, dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", p4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 10. Outputs Index\n",
    "\n",
    "outputs = {\n",
    "    \"overall_backtest_csv\": str(ARTIFACTS / \"backtest_overall.csv\"),\n",
    "    \"monthly_backtest_csv\": str(ARTIFACTS / \"backtest_monthly.csv\"),\n",
    "    \"interval_coverage_json\": str(ARTIFACTS / \"interval_coverage.json\"),\n",
    "    \"policy_simulation_csv\": str(ARTIFACTS / \"policy_simulation.csv\"),\n",
    "    \"event_residuals_csv\": str(ARTIFACTS / \"event_residuals.csv\"),\n",
    "    \"feature_importance_csv\": str(ARTIFACTS / \"feature_importance.csv\"),\n",
    "    \"top_mispredictions_csv\": str(ARTIFACTS / \"top_mispredictions.csv\"),\n",
    "    \"fig_monthly_mae\": str(FIGS / \"monthly_mae.png\"),\n",
    "    \"fig_monthly_smape\": str(FIGS / \"monthly_smape.png\"),\n",
    "    \"fig_quantile_intervals\": str(FIGS / \"quantile_intervals.png\"),\n",
    "    \"fig_feature_importance\": str(FIGS / \"feature_importance.png\"),\n",
    "}\n",
    "print(json.dumps(outputs, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
